[2024-06-25 08:52:43,801] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
06/25/2024 08:52:54 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:22281
W0625 08:52:57.197000 139960714098496 torch/distributed/run.py:757] 
W0625 08:52:57.197000 139960714098496 torch/distributed/run.py:757] *****************************************
W0625 08:52:57.197000 139960714098496 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0625 08:52:57.197000 139960714098496 torch/distributed/run.py:757] *****************************************
[2024-06-25 08:53:06,028] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-25 08:53:06,028] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-25 08:53:06,028] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-25 08:53:06,029] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-25 08:53:06,029] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-25 08:53:06,029] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-25 08:53:06,029] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-25 08:53:06,029] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.

[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.

[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[93m [WARNING] [0m async_io: please install the libaio-dev package with apt

[93m [WARNING] [0m async_io: please install the libaio-dev package with apt[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.[93m [WARNING] [0m async_io: please install the libaio-dev package with apt



[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.

[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH

[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
06/25/2024 08:53:12 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
06/25/2024 08:53:12 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16
[INFO|tokenization_utils_base.py:2106] 2024-06-25 08:53:12,828 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2106] 2024-06-25 08:53:12,828 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2106] 2024-06-25 08:53:12,828 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2106] 2024-06-25 08:53:12,828 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2106] 2024-06-25 08:53:12,828 >> loading file tokenizer_config.json
06/25/2024 08:53:12 - INFO - llamafactory.data.template - Add pad token: </s>
06/25/2024 08:53:12 - INFO - llamafactory.data.loader - Loading dataset alpaca_en_demo.json...
06/25/2024 08:53:13 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
06/25/2024 08:53:13 - INFO - llamafactory.hparams.parser - Process rank: 6, device: cuda:6, n_gpu: 1, distributed training: True, compute dtype: torch.float16
06/25/2024 08:53:13 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
06/25/2024 08:53:13 - INFO - llamafactory.hparams.parser - Process rank: 7, device: cuda:7, n_gpu: 1, distributed training: True, compute dtype: torch.float16
06/25/2024 08:53:13 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
06/25/2024 08:53:13 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
06/25/2024 08:53:13 - INFO - llamafactory.hparams.parser - Process rank: 4, device: cuda:4, n_gpu: 1, distributed training: True, compute dtype: torch.float16
06/25/2024 08:53:13 - INFO - llamafactory.hparams.parser - Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, compute dtype: torch.float16
06/25/2024 08:53:13 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
06/25/2024 08:53:13 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16
06/25/2024 08:53:13 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
06/25/2024 08:53:13 - INFO - llamafactory.hparams.parser - Process rank: 5, device: cuda:5, n_gpu: 1, distributed training: True, compute dtype: torch.float16
06/25/2024 08:53:13 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
06/25/2024 08:53:13 - INFO - llamafactory.hparams.parser - Process rank: 3, device: cuda:3, n_gpu: 1, distributed training: True, compute dtype: torch.float16
06/25/2024 08:53:13 - INFO - llamafactory.data.template - Add pad token: </s>
06/25/2024 08:53:13 - INFO - llamafactory.data.template - Add pad token: </s>
06/25/2024 08:53:13 - INFO - llamafactory.data.template - Add pad token: </s>
06/25/2024 08:53:13 - INFO - llamafactory.data.template - Add pad token: </s>
06/25/2024 08:53:13 - INFO - llamafactory.data.template - Add pad token: </s>
06/25/2024 08:53:13 - INFO - llamafactory.data.template - Add pad token: </s>
06/25/2024 08:53:13 - INFO - llamafactory.data.template - Add pad token: </s>
Converting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 5839.80 examples/s]
06/25/2024 08:53:16 - INFO - llamafactory.data.loader - Loading dataset alpaca_en_demo.json...
06/25/2024 08:53:16 - INFO - llamafactory.data.loader - Loading dataset alpaca_en_demo.json...
06/25/2024 08:53:16 - INFO - llamafactory.data.loader - Loading dataset alpaca_en_demo.json...
06/25/2024 08:53:16 - INFO - llamafactory.data.loader - Loading dataset alpaca_en_demo.json...
06/25/2024 08:53:16 - INFO - llamafactory.data.loader - Loading dataset alpaca_en_demo.json...
06/25/2024 08:53:16 - INFO - llamafactory.data.loader - Loading dataset alpaca_en_demo.json...
06/25/2024 08:53:16 - INFO - llamafactory.data.loader - Loading dataset alpaca_en_demo.json...
Converting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 814/1000 [00:00<00:00, 8089.32 examples/s]Converting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 9730.10 examples/s]Converting format of dataset (num_proc=16):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 566/1000 [00:00<00:00, 5532.61 examples/s]Converting format of dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 5428.76 examples/s]
Converting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 5263.13 examples/s]
Converting format of dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 5298.50 examples/s]
Converting format of dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 565/1000 [00:00<00:00, 5551.56 examples/s]Converting format of dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 4644.69 examples/s]
Running tokenizer on dataset (num_proc=16):   6%|â–‹         | 63/1000 [00:00<00:03, 253.49 examples/s]Converting format of dataset (num_proc=16):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 938/1000 [00:00<00:00, 9328.79 examples/s]Converting format of dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 9963.97 examples/s]Converting format of dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 4779.78 examples/s]
Converting format of dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 5407.35 examples/s]
Running tokenizer on dataset (num_proc=16):  32%|â–ˆâ–ˆâ–ˆâ–      | 315/1000 [00:00<00:00, 932.18 examples/s]Converting format of dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 5401.79 examples/s]
Running tokenizer on dataset (num_proc=16):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 628/1000 [00:00<00:00, 1517.72 examples/s]Running tokenizer on dataset (num_proc=16):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 938/1000 [00:00<00:00, 1885.95 examples/s]Running tokenizer on dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 1359.96 examples/s]
input_ids:
[1, 518, 25580, 29962, 20355, 915, 263, 1889, 310, 3907, 907, 5547, 29889, 518, 29914, 25580, 29962, 29871, 341, 5086, 907, 5547, 338, 385, 4780, 322, 628, 14803, 1889, 29991, 2266, 526, 4331, 29899, 1609, 29899, 10568, 11994, 373, 920, 304, 1207, 963, 29901, 13, 13, 29896, 29889, 4007, 6967, 596, 2348, 1127, 10070, 29889, 1152, 6996, 907, 5547, 29892, 366, 29915, 645, 817, 29901, 29871, 29896, 18002, 599, 29899, 15503, 4220, 1652, 473, 29892, 29871, 29906, 29808, 29892, 29871, 29896, 29914, 29906, 18002, 27274, 29892, 29871, 29896, 29914, 29906, 18002, 4094, 29892, 29871, 29896, 29914, 29946, 734, 294, 1129, 265, 15795, 29892, 322, 29871, 29906, 6131, 1129, 787, 286, 2152, 287, 541, 357, 29889, 13, 13, 29906, 29889, 23478, 278, 10193, 29901, 512, 263, 2919, 24907, 12580, 29880, 29892, 377, 3873, 4208, 278, 1652, 473, 322, 278, 29808, 29889, 19295, 1474, 788, 278, 27274, 322, 4094, 29892, 23546, 5393, 21003, 304, 9801, 393, 727, 526, 694, 301, 17204, 29889, 3462, 15795, 322, 286, 2152, 287, 541, 357, 29892, 322, 6837, 1532, 29889, 13, 13, 29941, 29889, 2803, 278, 10193, 1791, 29901, 960, 366, 508, 29892, 1235, 278, 10193, 7845, 363, 385, 7234, 470, 577, 29889, 910, 674, 1371, 278, 1652, 473, 304, 6425, 11831, 278, 23904, 322, 1207, 278, 907, 5547, 901, 22707, 29889, 13, 13, 29946, 29889, 940, 271, 596, 7243, 29901, 4721, 354, 271, 263, 1661, 29899, 303, 860, 7243, 975, 18350, 12871, 29889, 12790, 368, 541, 357, 278, 7243, 470, 671, 7984, 292, 805, 764, 304, 5557, 278, 907, 5547, 515, 12070, 292, 29889, 13, 13, 29945, 29889, 6803, 278, 10193, 29901, 5293, 263, 11979, 280, 470, 263, 7540, 3864, 18002, 29892, 1671, 263, 2319, 5253, 310, 10193, 313, 12717, 29871, 29896, 29914, 29946, 18002, 29897, 11480, 278, 4818, 310, 278, 7243, 29889, 1954, 4210, 2486, 260, 2782, 278, 7243, 297, 263, 19308, 10884, 304, 9677, 278, 10193, 1584, 368, 322, 16835, 368, 975, 278, 5970, 310, 278, 7243, 29889, 13, 13, 29953, 29889, 17278, 278, 907, 412, 29901, 17278, 278, 907, 412, 363, 29871, 29896, 29899, 29906, 6233, 2745, 278, 5970, 338, 3578, 368, 22843, 29889, 10057, 3730, 658, 7749, 278, 12770, 411, 263, 805, 271, 2497, 322, 285, 3466, 278, 907, 412, 975, 304, 7984, 278, 916, 2625, 363, 1790, 11015, 29889, 13, 13, 29955, 29889, 15154, 322, 12312, 29901, 402, 2705, 20343, 278, 907, 412, 11480, 263, 15284, 29892, 322, 769, 12312, 278, 1889, 411, 278, 9886, 10193, 29889, 22738, 304, 337, 29899, 4187, 357, 278, 7243, 1546, 1269, 907, 412, 565, 5181, 29889, 13, 13, 29947, 29889, 383, 453, 322, 9080, 29901, 383, 453, 596, 7984, 287, 907, 5547, 411, 596, 7429, 27523, 29892, 1316, 408, 10849, 15774, 29892, 377, 16242, 907, 314, 29892, 405, 329, 3547, 29892, 470, 16366, 322, 923, 968, 29889, 21809, 470, 900, 29881, 29892, 322, 9080, 7389, 29889, 1174, 2212, 29891, 29991, 29871, 2]
inputs:
<s> [INST] Describe a process of making crepes. [/INST]  Making crepes is an easy and delicious process! Here are step-by-step instructions on how to make them:

1. Assemble your ingredients. For basic crepes, you'll need: 1 cup all-purpose flour, 2 eggs, 1/2 cup milk, 1/2 cup water, 1/4 teaspoon salt, and 2 tablespoons melted butter.

2. Mix the batter: In a large mixing bowl, whisk together the flour and the eggs. Gradually add the milk and water, stirring constantly to ensure that there are no lumps. Add salt and melted butter, and mix well.

3. Let the batter rest: If you can, let the batter sit for an hour or so. This will help the flour to absorb the liquid and make the crepes more tender.

4. Heat your pan: Preheat a non-stick pan over medium heat. Lightly butter the pan or use cooking spray to prevent the crepes from sticking.

5. Pour the batter: Using a ladle or a measuring cup, pour a small amount of batter (about 1/4 cup) onto the center of the pan. Immediately tilt the pan in a circular motion to spread the batter evenly and thinly over the bottom of the pan.

6. Cook the crepe: Cook the crepe for 1-2 minutes until the bottom is lightly golden. Carefully loosen the edges with a spatula and flip the crepe over to cook the other side for another minute.

7. Remove and repeat: Gently slide the crepe onto a plate, and then repeat the process with the remaining batter. Remember to re-butter the pan between each crepe if necessary.

8. Fill and serve: Fill your cooked crepes with your desired filling, such as fresh fruit, whipped cream, Nutella, or ham and cheese. Roll or fold, and serve immediately. Enjoy! </s>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 29871, 341, 5086, 907, 5547, 338, 385, 4780, 322, 628, 14803, 1889, 29991, 2266, 526, 4331, 29899, 1609, 29899, 10568, 11994, 373, 920, 304, 1207, 963, 29901, 13, 13, 29896, 29889, 4007, 6967, 596, 2348, 1127, 10070, 29889, 1152, 6996, 907, 5547, 29892, 366, 29915, 645, 817, 29901, 29871, 29896, 18002, 599, 29899, 15503, 4220, 1652, 473, 29892, 29871, 29906, 29808, 29892, 29871, 29896, 29914, 29906, 18002, 27274, 29892, 29871, 29896, 29914, 29906, 18002, 4094, 29892, 29871, 29896, 29914, 29946, 734, 294, 1129, 265, 15795, 29892, 322, 29871, 29906, 6131, 1129, 787, 286, 2152, 287, 541, 357, 29889, 13, 13, 29906, 29889, 23478, 278, 10193, 29901, 512, 263, 2919, 24907, 12580, 29880, 29892, 377, 3873, 4208, 278, 1652, 473, 322, 278, 29808, 29889, 19295, 1474, 788, 278, 27274, 322, 4094, 29892, 23546, 5393, 21003, 304, 9801, 393, 727, 526, 694, 301, 17204, 29889, 3462, 15795, 322, 286, 2152, 287, 541, 357, 29892, 322, 6837, 1532, 29889, 13, 13, 29941, 29889, 2803, 278, 10193, 1791, 29901, 960, 366, 508, 29892, 1235, 278, 10193, 7845, 363, 385, 7234, 470, 577, 29889, 910, 674, 1371, 278, 1652, 473, 304, 6425, 11831, 278, 23904, 322, 1207, 278, 907, 5547, 901, 22707, 29889, 13, 13, 29946, 29889, 940, 271, 596, 7243, 29901, 4721, 354, 271, 263, 1661, 29899, 303, 860, 7243, 975, 18350, 12871, 29889, 12790, 368, 541, 357, 278, 7243, 470, 671, 7984, 292, 805, 764, 304, 5557, 278, 907, 5547, 515, 12070, 292, 29889, 13, 13, 29945, 29889, 6803, 278, 10193, 29901, 5293, 263, 11979, 280, 470, 263, 7540, 3864, 18002, 29892, 1671, 263, 2319, 5253, 310, 10193, 313, 12717, 29871, 29896, 29914, 29946, 18002, 29897, 11480, 278, 4818, 310, 278, 7243, 29889, 1954, 4210, 2486, 260, 2782, 278, 7243, 297, 263, 19308, 10884, 304, 9677, 278, 10193, 1584, 368, 322, 16835, 368, 975, 278, 5970, 310, 278, 7243, 29889, 13, 13, 29953, 29889, 17278, 278, 907, 412, 29901, 17278, 278, 907, 412, 363, 29871, 29896, 29899, 29906, 6233, 2745, 278, 5970, 338, 3578, 368, 22843, 29889, 10057, 3730, 658, 7749, 278, 12770, 411, 263, 805, 271, 2497, 322, 285, 3466, 278, 907, 412, 975, 304, 7984, 278, 916, 2625, 363, 1790, 11015, 29889, 13, 13, 29955, 29889, 15154, 322, 12312, 29901, 402, 2705, 20343, 278, 907, 412, 11480, 263, 15284, 29892, 322, 769, 12312, 278, 1889, 411, 278, 9886, 10193, 29889, 22738, 304, 337, 29899, 4187, 357, 278, 7243, 1546, 1269, 907, 412, 565, 5181, 29889, 13, 13, 29947, 29889, 383, 453, 322, 9080, 29901, 383, 453, 596, 7984, 287, 907, 5547, 411, 596, 7429, 27523, 29892, 1316, 408, 10849, 15774, 29892, 377, 16242, 907, 314, 29892, 405, 329, 3547, 29892, 470, 16366, 322, 923, 968, 29889, 21809, 470, 900, 29881, 29892, 322, 9080, 7389, 29889, 1174, 2212, 29891, 29991, 29871, 2]
labels:
 Making crepes is an easy and delicious process! Here are step-by-step instructions on how to make them:

1. Assemble your ingredients. For basic crepes, you'll need: 1 cup all-purpose flour, 2 eggs, 1/2 cup milk, 1/2 cup water, 1/4 teaspoon salt, and 2 tablespoons melted butter.

2. Mix the batter: In a large mixing bowl, whisk together the flour and the eggs. Gradually add the milk and water, stirring constantly to ensure that there are no lumps. Add salt and melted butter, and mix well.

3. Let the batter rest: If you can, let the batter sit for an hour or so. This will help the flour to absorb the liquid and make the crepes more tender.

4. Heat your pan: Preheat a non-stick pan over medium heat. Lightly butter the pan or use cooking spray to prevent the crepes from sticking.

5. Pour the batter: Using a ladle or a measuring cup, pour a small amount of batter (about 1/4 cup) onto the center of the pan. Immediately tilt the pan in a circular motion to spread the batter evenly and thinly over the bottom of the pan.

6. Cook the crepe: Cook the crepe for 1-2 minutes until the bottom is lightly golden. Carefully loosen the edges with a spatula and flip the crepe over to cook the other side for another minute.

7. Remove and repeat: Gently slide the crepe onto a plate, and then repeat the process with the remaining batter. Remember to re-butter the pan between each crepe if necessary.

8. Fill and serve: Fill your cooked crepes with your desired filling, such as fresh fruit, whipped cream, Nutella, or ham and cheese. Roll or fold, and serve immediately. Enjoy! </s>
[rank0]: Traceback (most recent call last):
[rank0]:   File "/ML-A800/home/zhangge/yicheng/LLaMA-Factory/src/llamafactory/launcher.py", line 9, in <module>
[rank0]:     launch()
[rank0]:   File "/ML-A800/home/zhangge/yicheng/LLaMA-Factory/src/llamafactory/launcher.py", line 5, in launch
[rank0]:     run_exp()
[rank0]:   File "/ML-A800/home/zhangge/yicheng/LLaMA-Factory/src/llamafactory/train/tuner.py", line 33, in run_exp
[rank0]:     run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)
[rank0]:   File "/ML-A800/home/zhangge/yicheng/LLaMA-Factory/src/llamafactory/train/sft/workflow.py", line 33, in run_sft
[rank0]:     dataset = get_dataset(model_args, data_args, training_args, stage="sft", **tokenizer_module)
[rank0]:   File "/ML-A800/home/zhangge/yicheng/LLaMA-Factory/src/llamafactory/data/loader.py", line 194, in get_dataset
[rank0]:     lengths[f'{key}_length'].append(len(example[key]))
[rank0]: KeyError: 'chosen_input_ids'
Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   0%|          | 0/1000 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|â–‹         | 63/1000 [00:00<00:02, 353.27 examples/s]Running tokenizer on dataset (num_proc=16):   6%|â–‹         | 63/1000 [00:00<00:02, 332.74 examples/s]Running tokenizer on dataset (num_proc=16):   6%|â–‹         | 63/1000 [00:00<00:02, 332.43 examples/s]Running tokenizer on dataset (num_proc=16):   6%|â–‹         | 63/1000 [00:00<00:02, 361.42 examples/s]Running tokenizer on dataset (num_proc=16):   6%|â–‹         | 63/1000 [00:00<00:03, 289.57 examples/s]Running tokenizer on dataset (num_proc=16):   6%|â–‹         | 63/1000 [00:00<00:03, 297.87 examples/s]Running tokenizer on dataset (num_proc=16):   6%|â–‹         | 63/1000 [00:00<00:03, 247.51 examples/s]Running tokenizer on dataset (num_proc=16):  25%|â–ˆâ–ˆâ–Œ       | 252/1000 [00:00<00:00, 891.14 examples/s]Running tokenizer on dataset (num_proc=16):  25%|â–ˆâ–ˆâ–Œ       | 252/1000 [00:00<00:00, 938.81 examples/s]Running tokenizer on dataset (num_proc=16):  32%|â–ˆâ–ˆâ–ˆâ–      | 315/1000 [00:00<00:00, 1152.33 examples/s]Running tokenizer on dataset (num_proc=16):  19%|â–ˆâ–‰        | 189/1000 [00:00<00:01, 680.08 examples/s]Running tokenizer on dataset (num_proc=16):  32%|â–ˆâ–ˆâ–ˆâ–      | 315/1000 [00:00<00:00, 1070.02 examples/s]Running tokenizer on dataset (num_proc=16):  25%|â–ˆâ–ˆâ–Œ       | 252/1000 [00:00<00:01, 726.15 examples/s]Running tokenizer on dataset (num_proc=16):  32%|â–ˆâ–ˆâ–ˆâ–      | 315/1000 [00:00<00:00, 876.52 examples/s]Running tokenizer on dataset (num_proc=16):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 441/1000 [00:00<00:00, 1150.75 examples/s]Running tokenizer on dataset (num_proc=16):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 566/1000 [00:00<00:00, 1498.00 examples/s]Running tokenizer on dataset (num_proc=16):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 504/1000 [00:00<00:00, 1328.97 examples/s]Running tokenizer on dataset (num_proc=16):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 441/1000 [00:00<00:00, 1257.27 examples/s]Running tokenizer on dataset (num_proc=16):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 504/1000 [00:00<00:00, 1177.66 examples/s]Running tokenizer on dataset (num_proc=16):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 378/1000 [00:00<00:00, 890.54 examples/s]Running tokenizer on dataset (num_proc=16):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 628/1000 [00:00<00:00, 1449.43 examples/s]Running tokenizer on dataset (num_proc=16):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 752/1000 [00:00<00:00, 1709.93 examples/s]Running tokenizer on dataset (num_proc=16):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 628/1000 [00:00<00:00, 1177.75 examples/s]Running tokenizer on dataset (num_proc=16):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 814/1000 [00:00<00:00, 1746.78 examples/s]Running tokenizer on dataset (num_proc=16):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 876/1000 [00:00<00:00, 1773.78 examples/s]Running tokenizer on dataset (num_proc=16):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 628/1000 [00:00<00:00, 1279.53 examples/s]Running tokenizer on dataset (num_proc=16):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 566/1000 [00:00<00:00, 1152.34 examples/s]Running tokenizer on dataset (num_proc=16):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 814/1000 [00:00<00:00, 1555.40 examples/s]Running tokenizer on dataset (num_proc=16):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 876/1000 [00:00<00:00, 1611.61 examples/s]Running tokenizer on dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 1681.67 examples/s]Running tokenizer on dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 1410.23 examples/s]
Running tokenizer on dataset (num_proc=16):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 938/1000 [00:00<00:00, 1505.70 examples/s]Running tokenizer on dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 1326.18 examples/s]
Running tokenizer on dataset (num_proc=16):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 938/1000 [00:00<00:00, 1651.88 examples/s]Running tokenizer on dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 1487.68 examples/s]Running tokenizer on dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 1276.64 examples/s]
Running tokenizer on dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 1201.55 examples/s]
Running tokenizer on dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 1241.56 examples/s]
Running tokenizer on dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 1161.74 examples/s]
Running tokenizer on dataset (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 1168.48 examples/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]W0625 08:53:22.231000 139960714098496 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 2899801 closing signal SIGTERM
W0625 08:53:22.232000 139960714098496 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 2899802 closing signal SIGTERM
W0625 08:53:22.232000 139960714098496 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 2899803 closing signal SIGTERM
W0625 08:53:22.232000 139960714098496 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 2899804 closing signal SIGTERM
W0625 08:53:22.233000 139960714098496 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 2899805 closing signal SIGTERM
W0625 08:53:22.233000 139960714098496 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 2899806 closing signal SIGTERM
W0625 08:53:22.233000 139960714098496 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 2899807 closing signal SIGTERM
E0625 08:53:23.027000 139960714098496 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: 1) local_rank: 0 (pid: 2899800) of binary: /ML-A100/team/mm/xw/anaconda3/envs/llama_factory/bin/python
Traceback (most recent call last):
  File "/ML-A100/team/mm/xw/anaconda3/envs/llama_factory/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/ML-A100/team/mm/xw/anaconda3/envs/llama_factory/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/ML-A100/team/mm/xw/anaconda3/envs/llama_factory/lib/python3.10/site-packages/torch/distributed/run.py", line 879, in main
    run(args)
  File "/ML-A100/team/mm/xw/anaconda3/envs/llama_factory/lib/python3.10/site-packages/torch/distributed/run.py", line 870, in run
    elastic_launch(
  File "/ML-A100/team/mm/xw/anaconda3/envs/llama_factory/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/ML-A100/team/mm/xw/anaconda3/envs/llama_factory/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 263, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/ML-A800/home/zhangge/yicheng/LLaMA-Factory/src/llamafactory/launcher.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-06-25_08:53:22
  host      : localhost
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2899800)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
